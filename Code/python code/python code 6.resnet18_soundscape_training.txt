# 训练集（终） python code 6.resnet18_soundscape_training
import os
import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torch.utils.tensorboard import SummaryWriter
from torchvision import models
from torchvision.transforms.functional import to_pil_image

torch.manual_seed(42)

# =============== 1. 定义多标签数据集 ===============
class NumpyDataset(Dataset):
    """
    加载多类(含组合) .npy 文件，并将标签转换为多热编码（multi-hot encoding）。
    基本标签: ["Anthrophony", "Avain", "Insect", "Geophony", "Silence"]
    示例:
      - "Anthrophony_Avain" → [1, 1, 0, 0, 0]
      - "Anthrophony_Avain_Insect" → [1, 1, 1, 0, 0]
      - "Silence" → [0, 0, 0, 0, 1]
    """
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform

        # 定义 5 个基本标签
        self.base_labels = ["Anthrophony", "Avain", "Insect", "Geophony", "Silence"]

        # 修改后的有效文件夹列表：重新加入 "Anthrophony_Avain_Insect"
        self.valid_folders = [
            "Anthrophony",
            "Avain",
            "Insect",
            "Geophony",
            "Silence",
            "Anthrophony_Avain",
            "Anthrophony_Geophony",
            "Anthrophony_Insect",
            "Avain_Geophony",
            "Avain_Insect",
            "Insect_Geophony",
            "Anthrophony_Avain_Insect"
        ]

        self.files = []
        for current_dir, _, files in os.walk(root_dir):
            folder_name = os.path.basename(current_dir)
            if folder_name in self.valid_folders:
                for f in files:
                    if f.lower().endswith('.npy'):
                        file_path = os.path.join(current_dir, f)
                        self.files.append(file_path)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        data = np.load(file_path)  # 假设 shape: (128, 376)

        # 扩展到 (3, 128, 376) 通道
        tensor_data = torch.from_numpy(data).float().unsqueeze(0)  # (1, 128, 376)
        tensor_data = tensor_data.repeat(3, 1, 1)                  # (3, 128, 376)

        # 如果需要插值到固定大小，可根据情况修改
        tensor_data = torch.nn.functional.interpolate(
            tensor_data.unsqueeze(0),
            size=(128, 376),
            mode='bilinear',
            align_corners=False
        ).squeeze(0)

        # 将文件夹名称拆分 → 多热编码
        folder_name = os.path.basename(os.path.dirname(file_path))
        label_vector = np.zeros(len(self.base_labels), dtype=np.float32)  # shape: (5,)

        if folder_name == "Silence":
            label_vector[self.base_labels.index("Silence")] = 1.0
        else:
            parts = folder_name.split('_')
            for part in parts:
                if part in self.base_labels:
                    idx_ = self.base_labels.index(part)
                    label_vector[idx_] = 1.0

        label_tensor = torch.from_numpy(label_vector).float()

        if self.transform:
            tensor_data = self.transform(tensor_data)

        return tensor_data, label_tensor

# =============== 2. 定义自定义余弦退火调度器（无 warm-up，带衰减因子） ===============
class CosineAnnealingRestartScheduler:
    """
    无 warmup 的余弦退火调度器，支持每次重启后学习率衰减 factor。
    更新后的逻辑为：在每个周期开始之前先更新 base_lr（乘以衰减因子），
    再根据当前周期内的进度计算学习率，这样每个新周期的第一个 epoch 就使用更新后的 base_lr。
    """
    def __init__(self, optimizer, T_max, base_lr=0.001, eta_min=0.0000001, factor=0.9):
        """
        :param optimizer: 优化器
        :param T_max: 一个余弦周期的 Epoch 数
        :param base_lr: 初始学习率
        :param eta_min: 最低学习率
        :param factor: 每次重启后 base_lr 的衰减因子
        """
        self.optimizer = optimizer
        self.T_max = T_max
        self.base_lr = base_lr
        self.eta_min = eta_min
        self.factor = factor

        self.epoch = 0  # 已完成的 epoch 数

        # 初始化时，将所有 param_group 的 lr 设为 base_lr
        for param_group in self.optimizer.param_groups:
            param_group["lr"] = self.base_lr

    def step(self):
        """
        每个 epoch 结束后调用一次，更新学习率。
        在新的周期开始前先更新 base_lr，再计算本周期内的学习率。
        """
        # 如果正好开始新的周期（除第一次外），先更新 base_lr
        if self.epoch % self.T_max == 0 and self.epoch != 0:
            self.base_lr *= self.factor

        cycle_progress = self.epoch % self.T_max  # 当前周期内的 epoch 进度
        cos_inner = math.pi * cycle_progress / self.T_max
        cos_out = (math.cos(cos_inner) + 1) / 2  # [0,1] 的余弦值

        # 计算当前学习率
        lr = self.eta_min + (self.base_lr - self.eta_min) * cos_out

        # 设置 optimizer 中的学习率
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        self.epoch += 1

    def get_lr(self):
        """
        返回当前学习率（若有多个 param_group，只返回第一个）
        """
        return [param_group['lr'] for param_group in self.optimizer.param_groups]

# =============== 3. 数据预处理与加载 ===============
data_transform = transforms.Compose([
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# 请修改为你的实际目录
train_dir = r"D:\pycharm\12345\LX_qiege_新分类_dataset_2\train"
val_dir   = r"D:\pycharm\12345\LX_qiege_新分类_dataset_2\val"

trainset = NumpyDataset(train_dir, transform=data_transform)
valset   = NumpyDataset(val_dir, transform=data_transform)

# =============== 3.1 过采样设置（WeightedRandomSampler） ===============
all_labels = []
for i in range(len(trainset)):
    _, label = trainset[i]
    all_labels.append(label.numpy())
all_labels = np.array(all_labels)  # shape: (N, 5)

# 统计每个标签出现次数
label_counts = np.sum(all_labels, axis=0)  # shape: (5,)
print("各基本标签样本数量：", label_counts)

# 计算每个标签的权重 (样本数少 → 权重越高)
label_weights = 1.0 / label_counts
print("各标签权重：", label_weights)

# 为每个样本计算平均权重
sample_weights = []
for label_vector in all_labels:
    if np.sum(label_vector) > 0:
        w = np.mean(label_weights[label_vector == 1])
    else:
        w = 1.0  # 理论上不会出现没有任何标签的样本
    sample_weights.append(w)
sample_weights = torch.tensor(sample_weights, dtype=torch.float)

sampler = WeightedRandomSampler(
    sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

# 训练集使用过采样
trainloader = DataLoader(trainset, batch_size=64, sampler=sampler)
# 验证集正常加载
valloader  = DataLoader(valset, batch_size=64, shuffle=False)

print("训练集样本数：", len(trainset))
print("验证集样本数：", len(valset))

# =============== 4. 定义模型、损失函数、优化器、调度器 ===============
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 构造 ResNet18 并修改输出层为 5 维 (多标签)
model = models.resnet18(weights=None)
model.fc = nn.Linear(model.fc.in_features, 5)  # 输出 5 个标签
model = model.to(device)

# 使用 BCEWithLogitsLoss 适用于多标签任务
criterion = nn.BCEWithLogitsLoss()

# 优化器
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 使用自定义余弦退火调度器（无 warmup + 每次重启衰减 factor=0.8）
scheduler = CosineAnnealingRestartScheduler(
    optimizer,
    T_max=25,             # 每个周期 25 个 epoch
    base_lr=0.01,         # 初始学习率
    eta_min=0.0000005,     # 最低学习率
    factor=0.8            # 每次重启后，base_lr 乘以 0.8
)

# =============== 5. 训练循环 ===============
num_epochs = 200
writer = SummaryWriter("./logs_resnet18_multilabel_new5")

train_losses = []
val_losses = []
# 改为样本级准确率：只有当每个样本所有标签均正确时才记为正确
train_accuracies = []
val_accuracies = []

best_val_loss = float('inf')
best_model_state = None

for epoch_idx in range(num_epochs):
    print(f"--------第{epoch_idx + 1}轮训练开始--------")
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)  # (batch_size, 5)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 计算样本级准确率：只有当一个样本的所有标签都预测正确时才记为正确
        preds = (torch.sigmoid(outputs) > 0.5).float()  # shape: (B, 5)
        sample_correct = torch.all(preds == labels, dim=1)  # shape: (B,)
        correct_train += sample_correct.sum().item()
        total_train += inputs.size(0)

        running_loss += loss.item()

    # 每个 epoch 结束后，调度器更新学习率
    scheduler.step()
    current_lr = scheduler.get_lr()[0]

    epoch_train_loss = running_loss / len(trainloader)
    epoch_train_acc = correct_train / total_train
    train_losses.append(epoch_train_loss)
    train_accuracies.append(epoch_train_acc)
    print(f"Epoch {epoch_idx + 1} - Train Loss: {epoch_train_loss:.4f}, "
          f"Train Acc: {epoch_train_acc:.4f}, LR: {current_lr:.6f}")

    # 验证阶段（样本级准确率）
    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0

    with torch.no_grad():
        for inputs, labels in valloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_val_loss += loss.item()

            preds = (torch.sigmoid(outputs) > 0.5).float()
            sample_correct = torch.all(preds == labels, dim=1)
            correct_val += sample_correct.sum().item()
            total_val += inputs.size(0)

    avg_val_loss = total_val_loss / len(valloader)
    avg_val_acc = correct_val / total_val
    val_losses.append(avg_val_loss)
    val_accuracies.append(avg_val_acc)
    print(f"Epoch {epoch_idx + 1} - Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}")

    # 保存验证集 loss 最低时的模型参数
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_model_state = model.state_dict()
        print(f"Epoch {epoch_idx + 1}: 保存最佳模型, 当前最佳验证 Loss: {best_val_loss:.4f}")

writer.close()
print("训练结束")

# 训练结束后保存最佳模型参数
torch.save(best_model_state, "best_model_multilabel_new5.pth")
print("最佳模型已保存")

# =============== 6. 绘制 Loss 和 Accuracy 对比图 ===============
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_losses) + 1), train_losses, label="Train Loss", marker='o')
plt.plot(range(1, len(val_losses) + 1), val_losses, label="Validation Loss", marker='o')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Train vs Validation Loss (Multi-label - 5 classes)")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label="Train Accuracy", marker='o')
plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label="Validation Accuracy", marker='o')
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Train vs Validation Accuracy (Multi-label - 5 classes)")
plt.legend()

plt.tight_layout()
plt.show()
