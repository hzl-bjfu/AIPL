# 独立测试集用（终）(英文版) python code 8.evaluate_out_sample_test
import os
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from torchvision import models

import matplotlib.pyplot as plt
import seaborn as sns
import math
import pandas as pd

from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    multilabel_confusion_matrix
)

##############################
# 1. 定义测试集 Dataset 类
##############################
class NumpyDatasetTestMultiLabel(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.base_labels = ["Anthrophony", "Avain", "Insect", "Geophony", "Silence"]

        self.valid_folders = [
            "Anthrophony", "Avain", "Insect", "Silence",
            "Anthrophony_Avain", "Anthrophony_Insect", "Avain_Insect",
            "Anthrophony_Avain_Insect", "Anthrophony_Insect_Geophony"
        ]

        self.files = []
        for current_dir, _, files in os.walk(root_dir):
            folder_name = os.path.basename(current_dir)
            if folder_name in self.valid_folders:
                for f in files:
                    if f.lower().endswith('.npy'):
                        self.files.append(os.path.join(current_dir, f))

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        data = np.load(file_path)
        tensor_data = torch.from_numpy(data).float().unsqueeze(0).repeat(3, 1, 1)
        tensor_data = torch.nn.functional.interpolate(
            tensor_data.unsqueeze(0), size=(128, 376),
            mode='bilinear', align_corners=False
        ).squeeze(0)

        folder_name = os.path.basename(os.path.dirname(file_path))
        label_vector = np.zeros(len(self.base_labels), dtype=np.float32)
        if folder_name == "Silence":
            label_vector[4] = 1.0
        else:
            for part in folder_name.split('_'):
                if part in self.base_labels:
                    label_vector[self.base_labels.index(part)] = 1.0

        if self.transform:
            tensor_data = self.transform(tensor_data)
        return tensor_data, torch.from_numpy(label_vector), os.path.basename(file_path)


##############################
# 2. 数据加载
##############################
test_transform = transforms.Compose([
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

test_dir = r"LX_独立test5_npy_精校"
testset = NumpyDatasetTestMultiLabel(test_dir, transform=test_transform)
testloader = DataLoader(testset, batch_size=64, shuffle=False)
print("测试集样本数：", len(testset))

##############################
# 3. 加载模型
##############################
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet18(weights=None)
model.fc = nn.Linear(model.fc.in_features, 5)
model.load_state_dict(torch.load("best_model_multilabel_new5.pth", map_location=device))
model.to(device).eval()

##############################
# 4. 计算指标函数
##############################
def compute_hamming_score(y_true, y_pred):
    scores = []
    for i in range(y_true.shape[0]):
        true_set = set(np.where(y_true[i]==1)[0])
        pred_set = set(np.where(y_pred[i]==1)[0])
        if not true_set and not pred_set:
            scores.append(1.0)
        else:
            scores.append(len(true_set & pred_set) / len(true_set | pred_set))
    return np.mean(scores)

def compute_hamming_loss(y_true, y_pred):
    return np.sum(y_true != y_pred) / y_true.size

##############################
# 5. 推理
##############################
all_preds, all_labels, all_filenames = [], [], []
with torch.no_grad():
    for inputs, labels, filenames in testloader:
        inputs = inputs.to(device)
        outputs = model(inputs)
        probs = torch.sigmoid(outputs).cpu().numpy()

        thresholds = {0:0.5, 1:0.5, 2:0.25, 3:0.5}
        silence_threshold = 0.5
        non_silence_idxs = [0,1,2,3]

        batch_preds = []
        for p in probs:
            pred = np.zeros_like(p, dtype=int)
            if any(p[i] >= thresholds[i] for i in non_silence_idxs):
                for i in non_silence_idxs:
                    pred[i] = int(p[i] >= thresholds[i])
                pred[4] = 0
            elif p[4] >= silence_threshold:
                pred[4] = 1
            else:
                pred[np.argmax(p)] = 1
            batch_preds.append(pred)

        all_preds.append(np.vstack(batch_preds))
        all_labels.append(labels.numpy())
        all_filenames.extend(filenames)

all_preds = np.vstack(all_preds)
all_labels = np.vstack(all_labels)

##############################
# 6. 计算并打印指标
##############################
hl = compute_hamming_loss(all_labels, all_preds)
hs = compute_hamming_score(all_labels, all_preds)
subset_acc = np.mean([np.array_equal(all_labels[i], all_preds[i]) for i in range(len(all_preds))])
print(f"[多标签] Hamming Loss: {hl:.4f}")
print(f"[多标签] Hamming Score: {hs:.4f}")
print(f"[多标签] Subset Accuracy: {subset_acc:.4f}")

##############################
# (B) 单标签混淆矩阵
##############################
base_class_names = ["Anthrophony", "Avain", "Insect", "Geophony", "Silence"]
mcm = multilabel_confusion_matrix(all_labels, all_preds, labels=[0,1,2,3,4])
plt.figure(figsize=(12,8))
for i, name in enumerate(base_class_names):
    plt.subplot(math.ceil(len(base_class_names)/2),2,i+1)
    sns.heatmap(mcm[i], annot=True, fmt="d", cmap="Blues")
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("True")
plt.tight_layout()
plt.show()

print("===== Per-label Classification Report =====")
print(classification_report(all_labels, all_preds, target_names=base_class_names, zero_division=0, digits=3))

##############################
# (D) 组合级混淆矩阵
##############################
combo_map = {
    (1,0,0,0,0): "Pure Anthrophony",
    (0,0,1,0,0): "Pure Insect",
    (0,1,0,0,0): "Pure Avian",
    (0,0,0,0,1): "Silence",
    (1,1,0,0,0): "Anthrophony + Avian",
    (1,0,1,0,0): "Anthrophony + Insect",
    (0,1,1,0,0): "Avian + Insect",
    (1,1,1,0,0): "Anthrophony + Avian + Insect"
}
def map_to_combo(vec):
    return combo_map.get(tuple(int(x) for x in vec), "Other")

mapped_preds = [map_to_combo(p) for p in all_preds]
mapped_labels = [map_to_combo(l) for l in all_labels]

combo_classes = [
    "Pure Anthrophony", "Pure Insect", "Pure Avian", "Silence",
    "Anthrophony + Avian", "Anthrophony + Insect",
    "Avian + Insect", "Anthrophony + Avian + Insect", "Other"
]

cm_combo = confusion_matrix(mapped_labels, mapped_preds, labels=combo_classes)
cm_pct = np.zeros_like(cm_combo, dtype=float)
for i, s in enumerate(cm_combo.sum(axis=1)):
    if s>0:
        cm_pct[i] = cm_combo[i]/s*100

plt.figure(figsize=(10,8))
sns.heatmap(cm_pct, annot=True, fmt=".1f", cmap="Blues",
            xticklabels=combo_classes, yticklabels=combo_classes)
plt.xticks(rotation=45, ha='right', fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel("Predicted label", fontsize=14)
plt.ylabel("True label", fontsize=14)
plt.tight_layout()
plt.show()

print("===== 组合级 Classification Report =====")
print(classification_report(mapped_labels, mapped_preds, labels=combo_classes, zero_division=0, digits=3))
combo_acc = np.mean([p==l for p,l in zip(mapped_preds, mapped_labels)])
print(f"[组合级] Accuracy: {combo_acc:.4f}")

##############################
# 7. 错误样本输出
##############################
def vec2str(vec, names):
    return ",".join([names[i] for i,v in enumerate(vec) if v==1])

errors = []
for fname, true_v, pred_v in zip(all_filenames, all_labels, all_preds):
    if not np.array_equal(true_v, pred_v):
        errors.append({
            'Filename': fname,
            'True label': vec2str(true_v, base_class_names),
            'Predicted label': vec2str(pred_v, base_class_names)
        })

if errors:
    pd.DataFrame(errors).to_excel('prediction_errors2.xlsx', index=False)
    print("错误样本已保存到：prediction_errors2.xlsx")
else:
    print("所有样本预测正确！")
